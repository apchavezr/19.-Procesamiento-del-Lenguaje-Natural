{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apchavezr/19.-Procesamiento-del-Lenguaje-Natural/blob/main/preprocesamiento_texto_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy-yCRCZ2Yns"
      },
      "source": [
        "# üìì Preprocesamiento de texto con NLTK\n",
        "\n",
        "Este notebook muestra paso a paso c√≥mo aplicar t√©cnicas b√°sicas de preprocesamiento de texto utilizando la librer√≠a **NLTK** en Python.\n",
        "\n",
        "**Objetivo**\n",
        "\n",
        "Implementar un flujo de preprocesamiento de texto en Python con NLTK, para que el estudiante comprenda y experimente de manera pr√°ctica las principales t√©cnicas aplicadas en PLN."
      ],
      "id": "Zy-yCRCZ2Yns"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmzli9Hc2Ynw",
        "outputId": "8b334154-446c-4265-ff0f-8339d78fc27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Instalar NLTK si no est√° en el entorno\n",
        "# !pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"
      ],
      "id": "Cmzli9Hc2Ynw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlzM0zrh2Yny"
      },
      "source": [
        "## 1. Texto de ejemplo"
      ],
      "id": "ZlzM0zrh2Yny"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDRx2GOq2Yn0",
        "outputId": "15447536-150e-4697-8263-948c38437f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original:\n",
            "Los estudiantes estaban corriendo r√°pidamente hacia la universidad, pero algunos ya hab√≠an corrido antes.\n"
          ]
        }
      ],
      "source": [
        "texto = \"Los estudiantes estaban corriendo r√°pidamente hacia la universidad, \\\n",
        "pero algunos ya hab√≠an corrido antes.\"\n",
        "\n",
        "print(\"Texto original:\")\n",
        "print(texto)"
      ],
      "id": "hDRx2GOq2Yn0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNV-t1uN2Yn1"
      },
      "source": [
        "## 2. Tokenizaci√≥n"
      ],
      "id": "dNV-t1uN2Yn1"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nBGBv4B2Yn2",
        "outputId": "1da5a467-9c83-4c3d-d6ea-3d8a08e2c456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens generados:\n",
            "['los', 'estudiantes', 'estaban', 'corriendo', 'r√°pidamente', 'hacia', 'la', 'universidad', ',', 'pero', 'algunos', 'ya', 'hab√≠an', 'corrido', 'antes', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(texto.lower(), language='spanish')\n",
        "print(\"Tokens generados:\")\n",
        "print(tokens)"
      ],
      "id": "1nBGBv4B2Yn2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9lwOrAk2Yn3"
      },
      "source": [
        "## 3. Eliminaci√≥n de stopwords"
      ],
      "id": "l9lwOrAk2Yn3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCQyAkC32Yn3",
        "outputId": "0e1db4f7-d274-4922-bbe2-5daf63fc1be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens sin stopwords:\n",
            "['estudiantes', 'corriendo', 'r√°pidamente', 'hacia', 'universidad', 'corrido']\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('spanish'))\n",
        "tokens_sin_stop = [palabra for palabra in tokens if palabra not in stop_words and palabra.isalpha()]\n",
        "print(\"Tokens sin stopwords:\")\n",
        "print(tokens_sin_stop)"
      ],
      "id": "tCQyAkC32Yn3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîé Observe c√≥mo la oraci√≥n se segmenta en palabras y signos de puntuaci√≥n."
      ],
      "metadata": {
        "id": "l5rmJn-w4xYJ"
      },
      "id": "l5rmJn-w4xYJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gbcy0Gq2Yn6"
      },
      "source": [
        "## 4. Stemming"
      ],
      "id": "5Gbcy0Gq2Yn6"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faZjfSuv2Yn7",
        "outputId": "77128ec6-0fca-4b1a-fa87-f0a784f8cf10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming aplicado:\n",
            "['estudi', 'corr', 'rapid', 'haci', 'univers', 'corr']\n"
          ]
        }
      ],
      "source": [
        "stemmer = SnowballStemmer('spanish')\n",
        "tokens_stem = [stemmer.stem(palabra) for palabra in tokens_sin_stop]\n",
        "print(\"Stemming aplicado:\")\n",
        "print(tokens_stem)"
      ],
      "id": "faZjfSuv2Yn7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîé Palabras como ‚Äúlos‚Äù, ‚Äúla‚Äù y ‚Äúpero‚Äù fueron eliminadas por ser stopwords."
      ],
      "metadata": {
        "id": "3wQK-hJ84tex"
      },
      "id": "3wQK-hJ84tex"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxgjZWFq2Yn8"
      },
      "source": [
        "## 5. Lematizaci√≥n"
      ],
      "id": "zxgjZWFq2Yn8"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX-YdJ1W2Yn9",
        "outputId": "580774e3-17df-4a32-d8cd-975dde9c4808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming aplicado:\n",
            "['estudi', 'corr', 'rapid', 'haci', 'univers', 'corr']\n"
          ]
        }
      ],
      "source": [
        "# Stemming con el algoritmo Snowball para espa√±ol\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "tokens_stem = [stemmer.stem(palabra) for palabra in tokens_sin_stop]\n",
        "\n",
        "print(\"Stemming aplicado:\")\n",
        "print(tokens_stem)\n"
      ],
      "id": "vX-YdJ1W2Yn9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîé Observe que algunas palabras se reducen a ra√≠ces como ‚Äúcorr‚Äù para ‚Äúcorriendo‚Äù y ‚Äúcorrido‚Äù."
      ],
      "metadata": {
        "id": "yX_8xo9u40cY"
      },
      "id": "yX_8xo9u40cY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Lematizaci√≥n (ejemplo en ingl√©s)\n",
        "\n",
        "NLTK no tiene un lematizador robusto en espa√±ol, por lo que se ejemplifica en ingl√©s:"
      ],
      "metadata": {
        "id": "nWqXaAVv5HX9"
      },
      "id": "nWqXaAVv5HX9"
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokens_en = [\"running\", \"ran\", \"runs\"]\n",
        "tokens_lem = [lemmatizer.lemmatize(pal, 'v') for pal in tokens_en]\n",
        "\n",
        "print(\"Lematizaci√≥n en ingl√©s:\")\n",
        "print(tokens_lem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8COTvk8k5G0z",
        "outputId": "5677f0b9-41a9-4822-c48f-88b798d5f649"
      },
      "id": "8COTvk8k5G0z",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lematizaci√≥n en ingl√©s:\n",
            "['run', 'run', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîé En este caso, todas las variantes verbales se reducen correctamente a ‚Äúrun‚Äù."
      ],
      "metadata": {
        "id": "4wGp0oVN5MKJ"
      },
      "id": "4wGp0oVN5MKJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd--1u472Yn-"
      },
      "source": [
        "## 7. Flujo completo"
      ],
      "id": "Nd--1u472Yn-"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNFlozbf2Yn-",
        "outputId": "e1bb64b5-1f5d-4b4c-e0e6-f5ac3cc7b18d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto procesado final (stemming aplicado):\n",
            "['estudi', 'corr', 'rapid', 'haci', 'univers', 'corr']\n"
          ]
        }
      ],
      "source": [
        "print(\"Texto procesado final (stemming aplicado):\")\n",
        "print(tokens_stem)"
      ],
      "id": "JNFlozbf2Yn-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gye_lZbd2Yn_"
      },
      "source": [
        "## Conclusiones  \n",
        "\n",
        "Al comparar los resultados de la eliminaci√≥n de stopwords, el stemming y la lematizaci√≥n, se evidencia que el vocabulario del texto se transforma de manera significativa. La eliminaci√≥n de stopwords reduce la cantidad de palabras al descartar t√©rminos poco informativos, el stemming recorta palabras a formas m√°s simples aunque a veces poco naturales, y la lematizaci√≥n devuelve la forma base correcta de cada t√©rmino. Estas diferencias muestran que la elecci√≥n de una t√©cnica u otra depende del objetivo del an√°lisis y del equilibrio entre precisi√≥n ling√º√≠stica y eficiencia computacional.\n",
        "\n",
        "üìö **Referencia**: Bird, S., Klein, E., & Loper, E. (2009). *Natural Language Processing with Python*. O‚ÄôReilly Media."
      ],
      "id": "Gye_lZbd2Yn_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}